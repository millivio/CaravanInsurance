{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9efca932-1271-4774-8867-65cb07f9d4d4",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "**Random Forest**\n",
    "- Random Forest is an **ensemble model** built from many decision trees.  \n",
    "- Each tree is trained on a random subset of the data and features (\"bagging\" + \"feature randomness\").  \n",
    "- The final prediction is made by **majority vote** (classification) or **averaging** (regression).  \n",
    "- This randomness makes the model more robust and reduces overfitting compared to a single decision tree.\n",
    "\n",
    "**Why ?**\n",
    "- Handles **non-linear relationships** and **feature interactions** automatically.  \n",
    "- Works well on mixed or categorical-like features without strict scaling.  \n",
    "- More powerful than Logistic Regression for complex patterns, are less sensitive to outliers than Logistic Regression.\n",
    "- Built-in support for **imbalanced classes** using `class_weight=\"balanced\"`.  \n",
    "\n",
    "It will help us see whether non-linear ensembles improve performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2caa264a-5538-499c-b1ee-c7258c75a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scripts.data_loader import load_caravan\n",
    "from scripts.metrics import evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d1e72ae-f1d4-42de-b2ae-161fd50cdcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, X, y, TARGET = load_caravan(data_dir=\"../data\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.30, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cdb91df-854d-42ba-9419-264ed4bba2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== random forest ===\n",
      "ROC-AUC: 0.7161 | PR-AUC: 0.1425\n",
      "Best-F1 threshold: 0.156\n",
      "At best-F1: Precision=0.155, Recall=0.500, F1=0.237\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89      1643\n",
      "           1       0.16      0.50      0.24       104\n",
      "\n",
      "    accuracy                           0.81      1747\n",
      "   macro avg       0.56      0.66      0.56      1747\n",
      "weighted avg       0.92      0.81      0.85      1747\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Class imbalance: use class_weight=\"balanced_subsample\" so each tree sees reweighted classes\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=400,           # number of trees\n",
    "    max_depth=None,             # allow trees to grow fully (we regularize via min_samples_leaf)\n",
    "    min_samples_leaf=2,         # small leaf size to reduce overfitting a bit\n",
    "    min_samples_split=5,        # avoid very tiny splits\n",
    "    class_weight=\"balanced_subsample\", #same as “balanced” except weights are computed based on the bootstrap sample for every tree grown.\n",
    "    n_jobs=-1,                  # use all CPU cores\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "proba_val = rf.predict_proba(X_val)[:, 1]\n",
    "preds_val = rf.predict(X_val)\n",
    "\n",
    "results = evaluate_model(\"random forest\", y_val, proba_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405a0f56-a586-4faa-8adb-e897980a95c0",
   "metadata": {},
   "source": [
    "## Random Forest Results\n",
    "\n",
    "**Global metrics**\n",
    "- **ROC-AUC = 0.716** → The model has a reasonable ability to rank buyers above non-buyers.  \n",
    "- **PR-AUC = 0.143** → On this imbalanced dataset (6% positives), this is about 2–3× better than random guessing (≈0.06).  \n",
    "\n",
    "**Performance at best-F1 threshold (0.156)**\n",
    "- **Precision = 0.155** → About 15% of predicted buyers are correct.  \n",
    "- **Recall = 0.500** → The model successfully identifies 50% of the true buyers.  \n",
    "- **F1 = 0.237** → Harmonic mean of precision and recall, balancing the two.  \n",
    "\n",
    "**Classification report (per class)**\n",
    "- **Class 0 (non-buyers)**: Precision 0.96, Recall 0.83 → the model is strong at detecting non-buyers.  \n",
    "- **Class 1 (buyers)**: Precision 0.16, Recall 0.50 → the model catches half of the buyers, but with many false positives.  \n",
    "\n",
    "**Accuracy = 0.81**  \n",
    "- Looks decent, but less informative here because the dataset is highly imbalanced.  \n",
    "\n",
    "**Interpretation**  \n",
    "- Random Forest captures **half of the true buyers** but at the cost of many false alarms.  \n",
    "- It performs better than random, but **precision is low**, meaning a marketing campaign would still contact many uninterested customers.  \n",
    "- Compared to Logistic Regression and Gradient Boosting, this model is **weaker in PR-AUC and F1**, so it is not the best choice overall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36f6328b-2ff0-48e0-ae74-d6346ca4f468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RF params: {'max_depth': 10, 'min_samples_leaf': 1, 'n_estimators': 200}\n",
      "Best CV PR-AUC: 0.1628\n",
      "\n",
      "=== random forest (tuned)  ===\n",
      "ROC-AUC: 0.7606 | PR-AUC: 0.1767\n",
      "Best-F1 threshold: 0.526\n",
      "At best-F1: Precision=0.259, Recall=0.288, F1=0.273\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95      1643\n",
      "           1       0.26      0.29      0.27       104\n",
      "\n",
      "    accuracy                           0.91      1747\n",
      "   macro avg       0.61      0.62      0.61      1747\n",
      "weighted avg       0.91      0.91      0.91      1747\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "rf_base = RandomForestClassifier(\n",
    "    class_weight=\"balanced_subsample\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [200, 400],\n",
    "    \"max_depth\": [None, 10],\n",
    "    \"min_samples_leaf\": [1, 3]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    estimator=rf_base,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"average_precision\",  # PR-AUC\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    "    return_train_score=False\n",
    ")\n",
    "\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best RF params:\", rf_grid.best_params_)\n",
    "print(\"Best CV PR-AUC:\", round(rf_grid.best_score_, 4))\n",
    "\n",
    "rf_best = rf_grid.best_estimator_\n",
    "\n",
    "# Evaluate tuned model on validation\n",
    "proba_val_best = rf_best.predict_proba(X_val)[:, 1]\n",
    "preds_val_best  = rf_best.predict(X_val)\n",
    "\n",
    "results = evaluate_model(\"random forest (tuned) \", y_val, proba_val_best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82668782-c8e3-4fad-b9cc-82bf5d98e607",
   "metadata": {},
   "source": [
    "## Random Forest (Tuned) Results\n",
    "\n",
    "- **ROC-AUC = 0.761** → The model can reasonably rank buyers vs. non-buyers.  \n",
    "- **PR-AUC = 0.177** → Better than random baseline (~0.06), but still modest for an imbalanced dataset.  \n",
    "\n",
    "- **Best-F1 threshold = 0.526**  \n",
    "  - **Precision = 0.259** → ~26% of predicted buyers are correct.  \n",
    "  - **Recall = 0.288** → ~29% of actual buyers are captured.  \n",
    "  - **F1 = 0.273** → Balanced score, better then logistic regression.  \n",
    "\n",
    "- **Class 0 (non-buyers)**: Very strong performance (precision/recall ≈ 0.95).  \n",
    "- **Class 1 (buyers)**: Weak recall and precision, reflecting the challenge of imbalance.  \n",
    "- **Accuracy = 0.91** looks high, but is dominated by the majority class and not a reliable indicator.  \n",
    "\n",
    "**Interpretation:**  \n",
    "Random Forest performs better than random guessing and provides a balanced baseline, but it struggles with recall on the minority class. Its overall performance (PR-AUC and F1) is better then logistic regression. But can be improved using gradient boosting. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
