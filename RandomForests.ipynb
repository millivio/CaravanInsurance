{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b72195a6-8179-4044-8638-18f066897bfc",
   "metadata": {},
   "source": [
    "Why Random Forests?\n",
    "Nonlinear, robust, handle mixed scales without scaling, capture interactions and nonlinearities, and are less sensitive to outliers than Logistic Regression.\n",
    "\n",
    "Key hyperparameters tuned:\n",
    "\n",
    "n_estimators (more trees reduce variance but cost time),\n",
    "\n",
    "max_depth (controls tree depth; limiting reduces overfit),\n",
    "\n",
    "min_samples_leaf (larger leaves make trees smoother/less overfit).\n",
    "Also used class_weight=\"balanced_subsample\" to mitigate class imbalance.\n",
    "\n",
    "Metrics:\n",
    "We keep PR-AUC as our main metric due to imbalance, plus Precision@Top-k% for the marketing use-case (who to contact).\n",
    "\n",
    "Interpretability:\n",
    "We showed Gini importances (fast) and optionally Permutation Importance (more reliable) to explain which features matter to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2caa264a-5538-499c-b1ee-c7258c75a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scripts.data_loader import load_caravan\n",
    "from scripts.metrics import evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d1e72ae-f1d4-42de-b2ae-161fd50cdcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, X, y, TARGET = load_caravan(data_dir=\"./data\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.30, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3cdb91df-854d-42ba-9419-264ed4bba2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== random forest ===\n",
      "ROC-AUC: 0.7161 | PR-AUC: 0.1425\n",
      "Best-F1 threshold: 0.156\n",
      "At best-F1: Precision=0.155, Recall=0.500, F1=0.237\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89      1643\n",
      "           1       0.16      0.50      0.24       104\n",
      "\n",
      "    accuracy                           0.81      1747\n",
      "   macro avg       0.56      0.66      0.56      1747\n",
      "weighted avg       0.92      0.81      0.85      1747\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Class imbalance: use class_weight=\"balanced_subsample\" so each tree sees reweighted classes\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=400,           # number of trees\n",
    "    max_depth=None,             # allow trees to grow fully (we regularize via min_samples_leaf)\n",
    "    min_samples_leaf=2,         # small leaf size to reduce overfitting a bit\n",
    "    min_samples_split=5,        # avoid very tiny splits\n",
    "    class_weight=\"balanced_subsample\",\n",
    "    n_jobs=-1,                  # use all CPU cores\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "proba_val = rf.predict_proba(X_val)[:, 1]\n",
    "preds_val = rf.predict(X_val)\n",
    "\n",
    "results = evaluate_model(\"random forest\", y_val, proba_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405a0f56-a586-4faa-8adb-e897980a95c0",
   "metadata": {},
   "source": [
    "ROC-AUC = 0.7168\n",
    "Overall ranking quality across all thresholds. It’s above random (0.5), so the RF has signal, but it’s weaker than Logistic Regression (~0.74).\n",
    "\n",
    "PR-AUC = 0.1381\n",
    "Area under the Precision–Recall curve. With a base rate ≈ 6% buyers, 0.138 is a modest lift over random, but worse than Logistic’s ~0.159. For imbalanced problems, PR-AUC is the more telling metric—RF is trailing here.\n",
    "\n",
    "At the default threshold (0.5)\n",
    "Confusion matrix\n",
    "TN=1062  FP=33\n",
    "FN=  64  TP= 6\n",
    "\n",
    "\n",
    "Class 1 (buyers): precision 0.15, recall 0.09, F1 0.11.\n",
    "Only 6 true positives out of 70 buyers are caught → the model is predicting “0” most of the time at the 0.5 cutoff.\n",
    "\n",
    "Accuracy 0.92 looks high but is misleading with imbalance (predicting 0’s already yields ~94%); accuracy isn’t a good KPI here.\n",
    "\n",
    "Campaign-style KPIs (more relevant)\n",
    "\n",
    "Precision@Top5% = 13.8%\n",
    "\n",
    "Precision@Top10% = 12.1%\n",
    "\n",
    "These beat the base rate (~6%), so there is lift—but it’s quite a bit below Logistic’s ~22.4% (Top5%) and ~19.8% (Top10%). That means RF’s ranking of prospects is currently worse than Logistic’s on this split.\n",
    "\n",
    "Why might RF underperform here?\n",
    "\n",
    "Default-ish hyperparameters: Even with class_weight=\"balanced_subsample\", RF may need tuning (e.g., max_depth, min_samples_leaf, n_estimators) to improve ranking.\n",
    "\n",
    "Feature nature: Many features are coded categories / ordinal buckets. Logistic (with scaling) sometimes does surprisingly well here, while a plain RF can overfit noise or fail to rank well without careful depth/leaf constraints.\n",
    "\n",
    "Thresholding: At 0.5, RF is extremely conservative (only 6 TPs). For this use-case, we usually don’t use a fixed 0.5. We either:\n",
    "\n",
    "pick a threshold that gives a desired precision/recall trade-off, or\n",
    "\n",
    "select the top-k% by probability (what you already report)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36f6328b-2ff0-48e0-ae74-d6346ca4f468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RF params: {'max_depth': 10, 'min_samples_leaf': 1, 'n_estimators': 200}\n",
      "Best CV PR-AUC: 0.1628\n",
      "\n",
      "=== random forest (tuned)  ===\n",
      "ROC-AUC: 0.7606 | PR-AUC: 0.1767\n",
      "Best-F1 threshold: 0.526\n",
      "At best-F1: Precision=0.259, Recall=0.288, F1=0.273\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95      1643\n",
      "           1       0.26      0.29      0.27       104\n",
      "\n",
      "    accuracy                           0.91      1747\n",
      "   macro avg       0.61      0.62      0.61      1747\n",
      "weighted avg       0.91      0.91      0.91      1747\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "rf_base = RandomForestClassifier(\n",
    "    class_weight=\"balanced_subsample\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [200, 400],\n",
    "    \"max_depth\": [None, 10],\n",
    "    \"min_samples_leaf\": [1, 3]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    estimator=rf_base,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"average_precision\",  # PR-AUC\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    "    return_train_score=False\n",
    ")\n",
    "\n",
    "# IMPORTANT: fit only on the TRAIN split to avoid leakage\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best RF params:\", rf_grid.best_params_)\n",
    "print(\"Best CV PR-AUC:\", round(rf_grid.best_score_, 4))\n",
    "\n",
    "rf_best = rf_grid.best_estimator_\n",
    "\n",
    "# Evaluate tuned model on validation\n",
    "proba_val_best = rf_best.predict_proba(X_val)[:, 1]\n",
    "preds_val_best  = rf_best.predict(X_val)\n",
    "\n",
    "results = evaluate_model(\"random forest (tuned) \", y_val, proba_val_best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92443e59-2c25-4a3c-a0e0-1152e17e614c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
